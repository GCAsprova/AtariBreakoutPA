{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUCGRfRrZ7Ie"
   },
   "source": [
    "# Dependancies"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "id": "WgPgswLfaAI_"
   },
   "source": [
    "#%pip install --upgrade --force-reinstall \"gymnasium[atari]\" autorom\n",
    "#%pip install \"gymnasium[other]\"\n",
    "#%pip install -r ../requirements.txt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMVasFexR24E"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sEE9paDaSLDR",
    "ExecuteTime": {
     "end_time": "2026-01-31T15:35:21.240502500Z",
     "start_time": "2026-01-31T15:35:18.839643500Z"
    }
   },
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import AtariPreprocessing, FrameStackObservation, TransformObservation\n",
    "from gymnasium.spaces import Box\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import ale_py\n",
    "\n",
    "#Need to adjust checkpoint and model saving when working locally instead of Colab env\n",
    "#from google.colab import files, drive"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDiRI1DfTrbO"
   },
   "source": [
    "#Setup and Hyperparameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8OpFs-GzU7fI",
    "ExecuteTime": {
     "end_time": "2026-01-31T15:35:30.737443400Z",
     "start_time": "2026-01-31T15:35:30.704767800Z"
    }
   },
   "source": [
    "#drive.mount(\"/content/drive\")\n",
    "\n",
    "# Training duration (frames)\n",
    "total_timesteps = 5_010_000\n",
    "max_steps_per_episode = 10_000\n",
    "\n",
    "# Replay buffer\n",
    "replay_buffer_size = 300_000\n",
    "learning_starts = 50_000\n",
    "\n",
    "# Optimization\n",
    "batch_size = 64\n",
    "learning_rate = 0.00005\n",
    "target_update_freq = 20_000   # steps\n",
    "\n",
    "# Exploration (epsilon-greedy)\n",
    "epsilon_start = 0.15\n",
    "epsilon_end = 0.1\n",
    "epsilon_decay_steps = 500_000\n",
    "epsilon_decay = (epsilon_start - epsilon_end) / epsilon_decay_steps\n",
    "\n",
    "# Gradient stabilization\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "#Other\n",
    "loss_function = keras.losses.Huber()\n",
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=max_grad_norm)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aLx06Qyd2vQz",
    "ExecuteTime": {
     "end_time": "2026-01-31T15:35:32.877375300Z",
     "start_time": "2026-01-31T15:35:32.861571600Z"
    }
   },
   "source": [
    "class FireResetWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "      super().__init__(env)\n",
    "      self.prev_lives = 0\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "      obs, info = self.env.reset(**kwargs)\n",
    "      # Update lives tracking on reset\n",
    "      self.prev_lives = info.get('lives', 0)\n",
    "\n",
    "      # Initial FIRE to start the game\n",
    "      obs, _, terminated, truncated, step_info = self.env.step(1)\n",
    "      if terminated or truncated:\n",
    "          return self.env.reset(**kwargs)\n",
    "\n",
    "      info.update(step_info)\n",
    "      return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "      obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "      # Check if we lost a life but the game is NOT over\n",
    "      current_lives = info.get('lives', 0)\n",
    "      if 0 < current_lives < self.prev_lives and not (terminated or truncated):\n",
    "        # The ball was lost, game is waiting for FIRE.\n",
    "        # We perform the FIRE step automatically.\n",
    "        obs, fire_reward, fire_term, fire_trunc, fire_info = self.env.step(1)\n",
    "\n",
    "        # Accumulate rewards/flags if the FIRE step somehow changes them\n",
    "        reward += fire_reward\n",
    "        terminated = terminated or fire_term\n",
    "        truncated = truncated or fire_trunc\n",
    "        info.update(fire_info)\n",
    "\n",
    "      self.prev_lives = current_lives\n",
    "      return obs, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "# --- High-Speed NumPy Replay Buffer ---\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, state_shape):\n",
    "        self.capacity = capacity\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "        self.states = np.empty((capacity, *state_shape), dtype=np.uint8)\n",
    "        self.next_states = np.empty((capacity, *state_shape), dtype=np.uint8)\n",
    "        self.actions = np.empty(capacity, dtype=np.int32)\n",
    "        self.rewards = np.empty(capacity, dtype=np.float32)\n",
    "        self.dones = np.empty(capacity, dtype=np.float32)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.states[self.ptr] = state\n",
    "        self.next_states[self.ptr] = next_state\n",
    "        self.actions[self.ptr] = action\n",
    "        self.rewards[self.ptr] = reward\n",
    "        self.dones[self.ptr] = done\n",
    "        self.ptr = (self.ptr + 1) % self.capacity\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "        return (self.states[idxs], self.next_states[idxs],\n",
    "                self.rewards[idxs], self.actions[idxs], self.dones[idxs])"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-02gxK7U04n"
   },
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HDes-Xe7TvdQ",
    "ExecuteTime": {
     "end_time": "2026-01-31T15:35:36.106466600Z",
     "start_time": "2026-01-31T15:35:35.980679100Z"
    }
   },
   "source": [
    "stack_frames = 4\n",
    "frame_skip = 4\n",
    "gamma = 0.99\n",
    "seed = 42\n",
    "num_actions = 4\n",
    "\n",
    "env = gym.make(\"ALE/Breakout-v5\", render_mode=\"rgb_array\",frameskip= 1)  # updated env name\n",
    "\n",
    "env = FireResetWrapper(env)\n",
    "\n",
    "# Preprocessing wrapper\n",
    "env = AtariPreprocessing(\n",
    "    env,\n",
    "    noop_max=30,\n",
    "    frame_skip=frame_skip,\n",
    "    terminal_on_life_loss=False,\n",
    "    grayscale_obs=True,\n",
    "    scale_obs=False\n",
    ")\n",
    "\n",
    "# Frame stacking wrapper\n",
    "env = FrameStackObservation(env, stack_size=stack_frames)\n",
    "\n",
    "# Define the new observation space for the transposed version (84, 84, 4)\n",
    "new_obs_space = Box(\n",
    "    low=0,\n",
    "    high=255,\n",
    "    shape=(84, 84, 4),\n",
    "    dtype=np.uint8\n",
    ")\n",
    "\n",
    "#Channels first to channels last\n",
    "env = TransformObservation(\n",
    "    env,\n",
    "    lambda obs: np.moveaxis(obs, 0, -1),\n",
    "    observation_space = new_obs_space\n",
    ")\n",
    "\n",
    "# Reset with seed\n",
    "obs, info = env.reset(seed=seed)\n",
    "\n",
    "print(type(obs),\"Observation shape:\", obs.shape)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> Observation shape: (84, 84, 4)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6fvaznBV9AF"
   },
   "source": [
    "# Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZK8uCKsvWFaB",
    "ExecuteTime": {
     "end_time": "2026-01-31T15:35:38.943753200Z",
     "start_time": "2026-01-31T15:35:38.532089600Z"
    }
   },
   "source": [
    "def create_q_model():\n",
    "    # Input is now (84, 84, 4) because of our env wrapper\n",
    "    inputs = layers.Input(shape=(84, 84, 4))\n",
    "\n",
    "    # Normalize 0-255 to 0-1\n",
    "    x = layers.Rescaling(1.0 / 255.0)(inputs)\n",
    "\n",
    "    # Convolutional layers (Standard DeepMind architecture)\n",
    "    x = layers.Conv2D(32, 8, strides=4, activation=\"relu\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Conv2D(64, 4, strides=2, activation=\"relu\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Conv2D(64, 3, strides=1, activation=\"relu\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Flatten()(x)\n",
    "\n",
    "    # --- Dueling Head ---\n",
    "    # 1. Advantage stream\n",
    "    adv = layers.Dense(512, activation=\"relu\", kernel_initializer=\"he_normal\")(x)\n",
    "    adv = layers.Dense(num_actions, activation=\"linear\")(adv)\n",
    "\n",
    "    # 2. Value stream\n",
    "    val = layers.Dense(512, activation=\"relu\", kernel_initializer=\"he_normal\")(x)\n",
    "    val = layers.Dense(1, activation=\"linear\")(val)\n",
    "\n",
    "    # Combine Advantage and Value: Q(s,a) = V(s) + (A(s,a) - Mean(A))\n",
    "    # This formula is mathematically more stable than simple addition\n",
    "    def combine_layer(args):\n",
    "        v, a = args\n",
    "        return v + (a - tf.reduce_mean(a, axis=1, keepdims=True))\n",
    "\n",
    "    outputs = layers.Lambda(combine_layer)([val, adv])\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Online and Target networks\n",
    "model = create_q_model()\n",
    "model_target = create_q_model()\n",
    "model_target.set_weights(model.get_weights())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\YaniPC\\Desktop\\TestingProject\\AtariTest\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2JH3LSQa-_06"
   },
   "source": [
    "#Search for Checkpoints and initialize Epsilon"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "m5XVlRef-_ki",
    "ExecuteTime": {
     "end_time": "2026-01-31T15:35:42.977363800Z",
     "start_time": "2026-01-31T15:35:42.957276300Z"
    }
   },
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def combine_layer(args):\n",
    "    v, a = args\n",
    "    return v + (a - tf.reduce_mean(a, axis=1, keepdims=True))\n",
    "\n",
    "# 1. Drive path to checkpoints\n",
    "checkpoint_dir = \"/content/drive/MyDrive/checkpoints3/\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# 2. Find the latest checkpoint\n",
    "checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.endswith('.keras') and 'dqn_' in f]\n",
    "\n",
    "if checkpoint_files:\n",
    "    # Sort files by frame number using regex\n",
    "    checkpoint_files.sort(key=lambda f: int(re.findall(r'\\d+', f)[0]), reverse=True)\n",
    "    latest_checkpoint = os.path.join(checkpoint_dir, checkpoint_files[0])\n",
    "\n",
    "    # Extract the frame count from the filename\n",
    "    frame_count = int(re.findall(r'\\d+', checkpoint_files[0])[0])\n",
    "\n",
    "    # Load weights into both models\n",
    "    print(f\"Resuming from {latest_checkpoint} at frame {frame_count}\")\n",
    "    model = keras.models.load_model(latest_checkpoint,custom_objects={'combine_layer': combine_layer})\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=max_grad_norm)\n",
    "\n",
    "    model_target = create_q_model()\n",
    "    model_target.set_weights(model.get_weights())\n",
    "\n",
    "    # Recalculate epsilon based on current frame_count\n",
    "    # This ensures agent doesn't start at 100% randomness again\n",
    "    epsilon = max(epsilon_end, epsilon_start - (frame_count * epsilon_decay))\n",
    "\n",
    "else:\n",
    "    print(\"No checkpoints found. Starting from scratch.\")\n",
    "    frame_count = 0\n",
    "    epsilon = epsilon_start"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoints found. Starting from scratch.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nnr8657WjUEA"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "n8VaNrCZ1akl",
    "ExecuteTime": {
     "end_time": "2026-01-31T15:35:51.034882700Z",
     "start_time": "2026-01-31T15:35:45.595853200Z"
    }
   },
   "source": [
    "buffer = ReplayBuffer(capacity=replay_buffer_size, state_shape=(84, 84, 4))\n",
    "\n",
    "episode_reward_history = []\n",
    "running_reward = 0.0\n",
    "episode_count = 0\n",
    "\n",
    "episode_reward_history = []\n",
    "running_reward = 0.0\n",
    "episode_count = 0\n",
    "starting_framecount = frame_count\n",
    "\n",
    "# @tf.function makes this run much faster by compiling it into a graph\n",
    "@tf.function\n",
    "def train_step(states, next_states, rewards, actions, dones, model, model_target, optimizer, loss_function, gamma):\n",
    "    # Just cast to float32; the model's Rescaling layer (1/255) handles the rest\n",
    "    states = tf.cast(states, tf.float32)\n",
    "    next_states = tf.cast(next_states, tf.float32)\n",
    "\n",
    "    # Double DQN Logic\n",
    "    next_q_online = model(next_states, training=False)\n",
    "    next_actions = tf.argmax(next_q_online, axis=1, output_type=tf.int32)\n",
    "\n",
    "    next_q_target = model_target(next_states, training=False)\n",
    "    indices = tf.stack([tf.range(tf.shape(next_actions)[0]), next_actions], axis=1)\n",
    "    next_q_values = tf.gather_nd(next_q_target, indices)\n",
    "\n",
    "    # Reward clipping (Standard for Atari to keep gradients stable)\n",
    "    clipped_rewards = tf.clip_by_value(rewards, -1.0, 1.0)\n",
    "    targets = clipped_rewards + gamma * (1.0 - dones) * next_q_values\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        q_values = model(states, training=True)\n",
    "        masks = tf.one_hot(actions, num_actions)\n",
    "        q_action = tf.reduce_sum(q_values * masks, axis=1)\n",
    "        loss = loss_function(targets, q_action)\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# -------------------------\n",
    "# Training Loop\n",
    "# -------------------------\n",
    "while frame_count < total_timesteps:\n",
    "    observation, info = env.reset()\n",
    "    # Ensure state is uint8 to save memory\n",
    "    state = np.array(observation, dtype=np.uint8)\n",
    "    episode_reward = 0\n",
    "\n",
    "    prev_lives = info.get('lives', 5) # Breakout starts with 5 lives\n",
    "\n",
    "    for step in range(1, max_steps_per_episode):\n",
    "        frame_count += 1\n",
    "\n",
    "        # --- Epsilon-Greedy Action ---\n",
    "        if frame_count < learning_starts or np.random.rand() < epsilon:\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            # Convert to float and normalize just for inference\n",
    "            state_tensor = tf.convert_to_tensor(state)\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "            state_tensor = tf.cast(state_tensor, tf.float32)\n",
    "\n",
    "            q_values = model(state_tensor, training=False)\n",
    "            action = tf.argmax(q_values[0]).numpy()\n",
    "\n",
    "        # Decay epsilon\n",
    "        epsilon -= epsilon_decay\n",
    "        epsilon = max(epsilon, epsilon_end)\n",
    "\n",
    "        # --- Environment Step ---\n",
    "        state_next, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        current_lives = info.get('lives', 0)\n",
    "        if current_lives < prev_lives:\n",
    "            # We \"overwrite\" the 0 reward with a -1 penalty\n",
    "            reward = -1.0\n",
    "\n",
    "        # Update lives for the next step comparison\n",
    "        prev_lives = current_lives\n",
    "\n",
    "        done = terminated or truncated\n",
    "        state_next = np.array(state_next, dtype=np.uint8)\n",
    "\n",
    "        episode_reward += reward\n",
    "\n",
    "        # --- Store in Buffer ---\n",
    "        buffer.add(state, action, reward, state_next, done)\n",
    "\n",
    "        state = state_next\n",
    "\n",
    "        # --- Train Step ---\n",
    "        if frame_count > learning_starts + starting_framecount and frame_count % 4 == 0:\n",
    "\n",
    "            state_sample, state_next_sample, rewards_sample, action_sample, done_sample = buffer.sample(batch_size)\n",
    "\n",
    "            # Call the optimized training function\n",
    "            train_step(\n",
    "              state_sample,\n",
    "              state_next_sample,\n",
    "              rewards_sample,\n",
    "              action_sample,\n",
    "              done_sample,\n",
    "              model,\n",
    "              model_target,\n",
    "              optimizer,\n",
    "              loss_function,\n",
    "              gamma\n",
    "          )\n",
    "\n",
    "        # --- Target Network Update ---\n",
    "        if frame_count % target_update_freq == 0:\n",
    "            model_target.set_weights(model.get_weights())\n",
    "            print(f\"Frames: {frame_count:,} | Episode: {episode_count} | \"\n",
    "                  f\"Reward(avg100): {running_reward:.2f} | Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "        # --- Checkpointing ---\n",
    "        if frame_count % 200_000 == 0:\n",
    "            print(f\"Saving checkpoint at {frame_count} frames\")\n",
    "            save_path = f\"/content/drive/MyDrive/checkpoints3/breakout_dqn_{frame_count}.keras\"\n",
    "            model.save(save_path)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # --- Episode End Updates ---\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    if len(episode_reward_history) > 100:\n",
    "        episode_reward_history.pop(0) # Standard list pop is fine here (short list)\n",
    "\n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "    episode_count += 1\n",
    "\n",
    "# Final Save\n",
    "model.save(\"/content/drive/MyDrive/checkpoints3/breakout_dqn_final.keras\")\n",
    "print(\"Training finished.\")"
   ],
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 57\u001B[39m\n\u001B[32m     55\u001B[39m \u001B[38;5;66;03m# --- Epsilon-Greedy Action ---\u001B[39;00m\n\u001B[32m     56\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m frame_count < learning_starts \u001B[38;5;129;01mor\u001B[39;00m np.random.rand() < epsilon:\n\u001B[32m---> \u001B[39m\u001B[32m57\u001B[39m     action = \u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrandom\u001B[49m\u001B[43m.\u001B[49m\u001B[43mchoice\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_actions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     58\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     59\u001B[39m     \u001B[38;5;66;03m# Convert to float and normalize just for inference\u001B[39;00m\n\u001B[32m     60\u001B[39m     state_tensor = tf.convert_to_tensor(state)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mnumpy/random/mtrand.pyx:1022\u001B[39m, in \u001B[36mnumpy.random.mtrand.RandomState.choice\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mnumpy/random/mtrand.pyx:799\u001B[39m, in \u001B[36mnumpy.random.mtrand.RandomState.randint\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mnumpy/random/_bounded_integers.pyx:2847\u001B[39m, in \u001B[36mnumpy.random._bounded_integers._rand_int32\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\TestingProject\\AtariTest\\.venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3287\u001B[39m, in \u001B[36mprod\u001B[39m\u001B[34m(a, axis, dtype, out, keepdims, initial, where)\u001B[39m\n\u001B[32m   3282\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_prod_dispatcher\u001B[39m(a, axis=\u001B[38;5;28;01mNone\u001B[39;00m, dtype=\u001B[38;5;28;01mNone\u001B[39;00m, out=\u001B[38;5;28;01mNone\u001B[39;00m, keepdims=\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   3283\u001B[39m                      initial=\u001B[38;5;28;01mNone\u001B[39;00m, where=\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m   3284\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m (a, out)\n\u001B[32m-> \u001B[39m\u001B[32m3287\u001B[39m \u001B[38;5;129m@array_function_dispatch\u001B[39m(_prod_dispatcher)\n\u001B[32m   3288\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mprod\u001B[39m(a, axis=\u001B[38;5;28;01mNone\u001B[39;00m, dtype=\u001B[38;5;28;01mNone\u001B[39;00m, out=\u001B[38;5;28;01mNone\u001B[39;00m, keepdims=np._NoValue,\n\u001B[32m   3289\u001B[39m          initial=np._NoValue, where=np._NoValue):\n\u001B[32m   3290\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   3291\u001B[39m \u001B[33;03m    Return the product of array elements over a given axis.\u001B[39;00m\n\u001B[32m   3292\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m   3402\u001B[39m \u001B[33;03m    10\u001B[39;00m\n\u001B[32m   3403\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m   3404\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m _wrapreduction(a, np.multiply, \u001B[33m'\u001B[39m\u001B[33mprod\u001B[39m\u001B[33m'\u001B[39m, axis, dtype, out,\n\u001B[32m   3405\u001B[39m                           keepdims=keepdims, initial=initial, where=where)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "toc_visible": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
