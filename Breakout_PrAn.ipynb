{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUCGRfRrZ7Ie"
      },
      "source": [
        "# Dependancies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "WgPgswLfaAI_",
        "outputId": "b58011e3-9aa8-480c-e92b-dfa804e3072c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting autorom\n",
            "  Using cached AutoROM-0.6.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting gymnasium[atari]\n",
            "  Using cached gymnasium-1.2.3-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting numpy>=1.21.0 (from gymnasium[atari])\n",
            "  Downloading numpy-2.4.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting cloudpickle>=1.2.0 (from gymnasium[atari])\n",
            "  Downloading cloudpickle-3.1.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting typing-extensions>=4.3.0 (from gymnasium[atari])\n",
            "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[atari])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Collecting ale_py>=0.9 (from gymnasium[atari])\n",
            "  Downloading ale_py-0.11.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting click (from autorom)\n",
            "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting requests (from autorom)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting charset_normalizer<4,>=2 (from requests->autorom)\n",
            "  Downloading charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->autorom)\n",
            "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->autorom)\n",
            "  Downloading urllib3-2.6.3-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests->autorom)\n",
            "  Downloading certifi-2026.1.4-py3-none-any.whl.metadata (2.5 kB)\n",
            "Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading ale_py-0.11.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cloudpickle-3.1.2-py3-none-any.whl (22 kB)\n",
            "Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Downloading numpy-2.4.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m136.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.3.1-py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.3/108.3 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gymnasium-1.2.3-py3-none-any.whl (952 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m952.1/952.1 kB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading certifi-2026.1.4-py3-none-any.whl (152 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.5/153.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.11-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-2.6.3-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.6/131.6 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: farama-notifications, urllib3, typing-extensions, numpy, idna, cloudpickle, click, charset_normalizer, certifi, requests, gymnasium, ale_py, autorom\n",
            "  Attempting uninstall: farama-notifications\n",
            "    Found existing installation: Farama-Notifications 0.0.4\n",
            "    Uninstalling Farama-Notifications-0.0.4:\n",
            "      Successfully uninstalled Farama-Notifications-0.0.4\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.5.0\n",
            "    Uninstalling urllib3-2.5.0:\n",
            "      Successfully uninstalled urllib3-2.5.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.15.0\n",
            "    Uninstalling typing_extensions-4.15.0:\n",
            "      Successfully uninstalled typing_extensions-4.15.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.11\n",
            "    Uninstalling idna-3.11:\n",
            "      Successfully uninstalled idna-3.11\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 3.1.2\n",
            "    Uninstalling cloudpickle-3.1.2:\n",
            "      Successfully uninstalled cloudpickle-3.1.2\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.3.1\n",
            "    Uninstalling click-8.3.1:\n",
            "      Successfully uninstalled click-8.3.1\n",
            "  Attempting uninstall: charset_normalizer\n",
            "    Found existing installation: charset-normalizer 3.4.4\n",
            "    Uninstalling charset-normalizer-3.4.4:\n",
            "      Successfully uninstalled charset-normalizer-3.4.4\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2026.1.4\n",
            "    Uninstalling certifi-2026.1.4:\n",
            "      Successfully uninstalled certifi-2026.1.4\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: gymnasium\n",
            "    Found existing installation: gymnasium 1.2.3\n",
            "    Uninstalling gymnasium-1.2.3:\n",
            "      Successfully uninstalled gymnasium-1.2.3\n",
            "  Attempting uninstall: ale_py\n",
            "    Found existing installation: ale-py 0.11.2\n",
            "    Uninstalling ale-py-0.11.2:\n",
            "      Successfully uninstalled ale-py-0.11.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.4.1 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ale_py-0.11.2 autorom-0.6.1 certifi-2026.1.4 charset_normalizer-3.4.4 click-8.3.1 cloudpickle-3.1.2 farama-notifications-0.0.4 gymnasium-1.2.3 idna-3.11 numpy-2.4.1 requests-2.32.5 typing-extensions-4.15.0 urllib3-2.6.3\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "ad384a3b6fcc40fcad0e42c287052afa",
              "pip_warning": {
                "packages": [
                  "certifi",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%pip install --upgrade --force-reinstall \"gymnasium[atari]\" autorom"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMVasFexR24E"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEE9paDaSLDR",
        "outputId": "0e58c971-2b32-4c6d-f64a-0a8ba7783077"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ale-py version 0.11.2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import keras\n",
        "from keras import layers\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import AtariPreprocessing, FrameStackObservation, TransformObservation\n",
        "from gymnasium.spaces import Box\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import ale_py\n",
        "print(\"ale-py version\", ale_py.__version__)\n",
        "\n",
        "from google.colab import files, drive\n",
        "from collections import deque\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDiRI1DfTrbO"
      },
      "source": [
        "#Setup and Hyperparameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OpFs-GzU7fI",
        "outputId": "9bb9765f-f9f7-47f6-8a9a-3d7d2c2deee5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "\n",
        "# Training duration (frames)\n",
        "total_timesteps = 5_010_000\n",
        "\n",
        "# Replay buffer\n",
        "replay_buffer_size = 300_000\n",
        "learning_starts = 50_000\n",
        "\n",
        "# Optimization\n",
        "batch_size = 64\n",
        "learning_rate = 0.00005\n",
        "target_update_freq = 20_000   # steps\n",
        "\n",
        "# Exploration (epsilon-greedy)\n",
        "epsilon_start = 0.15\n",
        "epsilon_end = 0.1\n",
        "epsilon_decay_steps = 500_000\n",
        "epsilon_decay = (epsilon_start - epsilon_end) / epsilon_decay_steps\n",
        "\n",
        "# Gradient stabilization\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "#Other\n",
        "loss_function = keras.losses.Huber()\n",
        "optimizer = keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=max_grad_norm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLx06Qyd2vQz"
      },
      "outputs": [],
      "source": [
        "class FireResetWrapper(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "      super().__init__(env)\n",
        "      self.prev_lives = 0\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "      obs, info = self.env.reset(**kwargs)\n",
        "      # Update lives tracking on reset\n",
        "      self.prev_lives = info.get('lives', 0)\n",
        "\n",
        "      # Initial FIRE to start the game\n",
        "      obs, _, terminated, truncated, step_info = self.env.step(1)\n",
        "      if terminated or truncated:\n",
        "          return self.env.reset(**kwargs)\n",
        "\n",
        "      info.update(step_info)\n",
        "      return obs, info\n",
        "\n",
        "    def step(self, action):\n",
        "      obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "\n",
        "      # Check if we lost a life but the game is NOT over\n",
        "      current_lives = info.get('lives', 0)\n",
        "      if 0 < current_lives < self.prev_lives and not (terminated or truncated):\n",
        "        # The ball was lost, game is waiting for FIRE.\n",
        "        # We perform the FIRE step automatically.\n",
        "        obs, fire_reward, fire_term, fire_trunc, fire_info = self.env.step(1)\n",
        "\n",
        "        # Accumulate rewards/flags if the FIRE step somehow changes them\n",
        "        reward += fire_reward\n",
        "        terminated = terminated or fire_term\n",
        "        truncated = truncated or fire_trunc\n",
        "        info.update(fire_info)\n",
        "\n",
        "      self.prev_lives = current_lives\n",
        "      return obs, reward, terminated, truncated, info\n",
        "\n",
        "\n",
        "# --- High-Speed NumPy Replay Buffer ---\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity, state_shape):\n",
        "        self.capacity = capacity\n",
        "        self.ptr = 0\n",
        "        self.size = 0\n",
        "        self.states = np.empty((capacity, *state_shape), dtype=np.uint8)\n",
        "        self.next_states = np.empty((capacity, *state_shape), dtype=np.uint8)\n",
        "        self.actions = np.empty(capacity, dtype=np.int32)\n",
        "        self.rewards = np.empty(capacity, dtype=np.float32)\n",
        "        self.dones = np.empty(capacity, dtype=np.float32)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        self.states[self.ptr] = state\n",
        "        self.next_states[self.ptr] = next_state\n",
        "        self.actions[self.ptr] = action\n",
        "        self.rewards[self.ptr] = reward\n",
        "        self.dones[self.ptr] = done\n",
        "        self.ptr = (self.ptr + 1) % self.capacity\n",
        "        self.size = min(self.size + 1, self.capacity)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
        "        return (self.states[idxs], self.next_states[idxs],\n",
        "                self.rewards[idxs], self.actions[idxs], self.dones[idxs])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-02gxK7U04n"
      },
      "source": [
        "# Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDes-Xe7TvdQ",
        "outputId": "38453597-3bae-4a39-80fe-6898d62b2a8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'> Observation shape: (84, 84, 4)\n"
          ]
        }
      ],
      "source": [
        "stack_frames = 4\n",
        "frame_skip = 4\n",
        "gamma = 0.99\n",
        "seed = 42\n",
        "num_actions = 4\n",
        "\n",
        "env = gym.make(\"ALE/Breakout-v5\", render_mode=\"rgb_array\",frameskip= 1)  # updated env name\n",
        "\n",
        "env = FireResetWrapper(env)\n",
        "\n",
        "# Preprocessing wrapper\n",
        "env = AtariPreprocessing(\n",
        "    env,\n",
        "    noop_max=30,\n",
        "    frame_skip=frame_skip,\n",
        "    terminal_on_life_loss=False,\n",
        "    grayscale_obs=True,\n",
        "    scale_obs=False\n",
        ")\n",
        "\n",
        "# Frame stacking wrapper\n",
        "env = FrameStackObservation(env, stack_size=stack_frames)\n",
        "\n",
        "# Define the new observation space for the transposed version (84, 84, 4)\n",
        "new_obs_space = Box(\n",
        "    low=0,\n",
        "    high=255,\n",
        "    shape=(84, 84, 4),\n",
        "    dtype=np.uint8\n",
        ")\n",
        "\n",
        "#Channels first to channels last\n",
        "env = TransformObservation(\n",
        "    env,\n",
        "    lambda obs: np.moveaxis(obs, 0, -1),\n",
        "    observation_space = new_obs_space\n",
        ")\n",
        "\n",
        "# Reset with seed\n",
        "obs, info = env.reset(seed=seed)\n",
        "\n",
        "print(type(obs),\"Observation shape:\", obs.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6fvaznBV9AF"
      },
      "source": [
        "# Deep Q-Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZK8uCKsvWFaB"
      },
      "outputs": [],
      "source": [
        "def create_q_model():\n",
        "    # Input is now (84, 84, 4) because of our env wrapper\n",
        "    inputs = layers.Input(shape=(84, 84, 4))\n",
        "\n",
        "    # Normalize 0-255 to 0-1\n",
        "    x = layers.Rescaling(1.0 / 255.0)(inputs)\n",
        "\n",
        "    # Convolutional layers (Standard DeepMind architecture)\n",
        "    x = layers.Conv2D(32, 8, strides=4, activation=\"relu\", kernel_initializer=\"he_normal\")(x)\n",
        "    x = layers.Conv2D(64, 4, strides=2, activation=\"relu\", kernel_initializer=\"he_normal\")(x)\n",
        "    x = layers.Conv2D(64, 3, strides=1, activation=\"relu\", kernel_initializer=\"he_normal\")(x)\n",
        "    x = layers.Flatten()(x)\n",
        "\n",
        "    # --- Dueling Head ---\n",
        "    # 1. Advantage stream\n",
        "    adv = layers.Dense(512, activation=\"relu\", kernel_initializer=\"he_normal\")(x)\n",
        "    adv = layers.Dense(num_actions, activation=\"linear\")(adv)\n",
        "\n",
        "    # 2. Value stream\n",
        "    val = layers.Dense(512, activation=\"relu\", kernel_initializer=\"he_normal\")(x)\n",
        "    val = layers.Dense(1, activation=\"linear\")(val)\n",
        "\n",
        "    # Combine Advantage and Value: Q(s,a) = V(s) + (A(s,a) - Mean(A))\n",
        "    # This formula is mathematically more stable than simple addition\n",
        "    def combine_layer(args):\n",
        "        v, a = args\n",
        "        return v + (a - tf.reduce_mean(a, axis=1, keepdims=True))\n",
        "\n",
        "    outputs = layers.Lambda(combine_layer)([val, adv])\n",
        "\n",
        "    return keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Online and Target networks\n",
        "model = create_q_model()\n",
        "model_target = create_q_model()\n",
        "model_target.set_weights(model.get_weights())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JH3LSQa-_06"
      },
      "source": [
        "#Search for Checkpoints and initialize Epsilon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5XVlRef-_ki",
        "outputId": "af1a3a12-8577-43bd-d7a7-0fa107371f93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuming from /content/drive/MyDrive/checkpoints3/breakout_dqn_3800000.keras at frame 3800000\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "def combine_layer(args):\n",
        "    v, a = args\n",
        "    return v + (a - tf.reduce_mean(a, axis=1, keepdims=True))\n",
        "\n",
        "# 1. Drive path to checkpoints\n",
        "checkpoint_dir = \"/content/drive/MyDrive/checkpoints3/\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# 2. Find the latest checkpoint\n",
        "checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.endswith('.keras') and 'dqn_' in f]\n",
        "\n",
        "if checkpoint_files:\n",
        "    # Sort files by frame number using regex\n",
        "    checkpoint_files.sort(key=lambda f: int(re.findall(r'\\d+', f)[0]), reverse=True)\n",
        "    latest_checkpoint = os.path.join(checkpoint_dir, checkpoint_files[0])\n",
        "\n",
        "    # Extract the frame count from the filename\n",
        "    frame_count = int(re.findall(r'\\d+', checkpoint_files[0])[0])\n",
        "\n",
        "    # Load weights into both models\n",
        "    print(f\"Resuming from {latest_checkpoint} at frame {frame_count}\")\n",
        "    model = keras.models.load_model(latest_checkpoint,custom_objects={'combine_layer': combine_layer})\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=max_grad_norm)\n",
        "\n",
        "    model_target = create_q_model()\n",
        "    model_target.set_weights(model.get_weights())\n",
        "\n",
        "    # Recalculate epsilon based on current frame_count\n",
        "    # This ensures agent doesn't start at 100% randomness again\n",
        "    epsilon = max(epsilon_end, epsilon_start - (frame_count * epsilon_decay))\n",
        "\n",
        "else:\n",
        "    print(\"No checkpoints found. Starting from scratch.\")\n",
        "    frame_count = 0\n",
        "    epsilon = epsilon_start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nnr8657WjUEA"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8VaNrCZ1akl",
        "outputId": "b4702748-8207-4806-db49-6635a5216960"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frames: 3,820,000 | Episode: 18 | Reward(avg100): 39.44 | Epsilon: 0.100\n",
            "Frames: 3,840,000 | Episode: 37 | Reward(avg100): 37.38 | Epsilon: 0.100\n",
            "Frames: 3,860,000 | Episode: 56 | Reward(avg100): 36.48 | Epsilon: 0.100\n",
            "Frames: 3,880,000 | Episode: 80 | Reward(avg100): 32.15 | Epsilon: 0.100\n",
            "Frames: 3,900,000 | Episode: 100 | Reward(avg100): 31.56 | Epsilon: 0.100\n",
            "Frames: 3,920,000 | Episode: 121 | Reward(avg100): 30.61 | Epsilon: 0.100\n",
            "Frames: 3,940,000 | Episode: 141 | Reward(avg100): 28.82 | Epsilon: 0.100\n",
            "Frames: 3,960,000 | Episode: 163 | Reward(avg100): 28.89 | Epsilon: 0.100\n",
            "Frames: 3,980,000 | Episode: 183 | Reward(avg100): 29.91 | Epsilon: 0.100\n",
            "Frames: 4,000,000 | Episode: 205 | Reward(avg100): 30.44 | Epsilon: 0.100\n",
            "Saving checkpoint at 4000000 frames\n",
            "Frames: 4,020,000 | Episode: 227 | Reward(avg100): 28.29 | Epsilon: 0.100\n",
            "Frames: 4,040,000 | Episode: 248 | Reward(avg100): 28.38 | Epsilon: 0.100\n",
            "Frames: 4,060,000 | Episode: 269 | Reward(avg100): 29.97 | Epsilon: 0.100\n",
            "Frames: 4,080,000 | Episode: 289 | Reward(avg100): 30.27 | Epsilon: 0.100\n",
            "Frames: 4,100,000 | Episode: 313 | Reward(avg100): 28.94 | Epsilon: 0.100\n",
            "Frames: 4,120,000 | Episode: 334 | Reward(avg100): 29.19 | Epsilon: 0.100\n",
            "Frames: 4,140,000 | Episode: 352 | Reward(avg100): 29.52 | Epsilon: 0.100\n",
            "Frames: 4,160,000 | Episode: 372 | Reward(avg100): 29.10 | Epsilon: 0.100\n",
            "Frames: 4,180,000 | Episode: 392 | Reward(avg100): 30.20 | Epsilon: 0.100\n",
            "Frames: 4,200,000 | Episode: 411 | Reward(avg100): 32.28 | Epsilon: 0.100\n",
            "Saving checkpoint at 4200000 frames\n",
            "Frames: 4,220,000 | Episode: 428 | Reward(avg100): 33.79 | Epsilon: 0.100\n",
            "Frames: 4,240,000 | Episode: 449 | Reward(avg100): 34.10 | Epsilon: 0.100\n",
            "Frames: 4,260,000 | Episode: 466 | Reward(avg100): 35.94 | Epsilon: 0.100\n",
            "Frames: 4,280,000 | Episode: 486 | Reward(avg100): 36.44 | Epsilon: 0.100\n",
            "Frames: 4,300,000 | Episode: 504 | Reward(avg100): 37.02 | Epsilon: 0.100\n",
            "Frames: 4,320,000 | Episode: 524 | Reward(avg100): 35.79 | Epsilon: 0.100\n",
            "Frames: 4,340,000 | Episode: 542 | Reward(avg100): 36.24 | Epsilon: 0.100\n",
            "Frames: 4,360,000 | Episode: 561 | Reward(avg100): 37.35 | Epsilon: 0.100\n",
            "Frames: 4,380,000 | Episode: 581 | Reward(avg100): 34.88 | Epsilon: 0.100\n",
            "Frames: 4,400,000 | Episode: 601 | Reward(avg100): 34.50 | Epsilon: 0.100\n",
            "Saving checkpoint at 4400000 frames\n",
            "Frames: 4,420,000 | Episode: 623 | Reward(avg100): 33.55 | Epsilon: 0.100\n",
            "Frames: 4,440,000 | Episode: 642 | Reward(avg100): 33.56 | Epsilon: 0.100\n",
            "Frames: 4,460,000 | Episode: 660 | Reward(avg100): 34.03 | Epsilon: 0.100\n",
            "Frames: 4,480,000 | Episode: 677 | Reward(avg100): 35.94 | Epsilon: 0.100\n",
            "Frames: 4,500,000 | Episode: 696 | Reward(avg100): 36.96 | Epsilon: 0.100\n",
            "Frames: 4,520,000 | Episode: 715 | Reward(avg100): 37.63 | Epsilon: 0.100\n",
            "Frames: 4,540,000 | Episode: 732 | Reward(avg100): 40.29 | Epsilon: 0.100\n",
            "Frames: 4,560,000 | Episode: 750 | Reward(avg100): 41.00 | Epsilon: 0.100\n",
            "Frames: 4,580,000 | Episode: 768 | Reward(avg100): 41.00 | Epsilon: 0.100\n",
            "Frames: 4,600,000 | Episode: 786 | Reward(avg100): 39.94 | Epsilon: 0.100\n",
            "Saving checkpoint at 4600000 frames\n",
            "Frames: 4,620,000 | Episode: 807 | Reward(avg100): 39.42 | Epsilon: 0.100\n",
            "Frames: 4,640,000 | Episode: 828 | Reward(avg100): 36.29 | Epsilon: 0.100\n",
            "Frames: 4,660,000 | Episode: 848 | Reward(avg100): 35.14 | Epsilon: 0.100\n",
            "Frames: 4,680,000 | Episode: 869 | Reward(avg100): 32.20 | Epsilon: 0.100\n",
            "Frames: 4,700,000 | Episode: 888 | Reward(avg100): 30.33 | Epsilon: 0.100\n",
            "Frames: 4,720,000 | Episode: 908 | Reward(avg100): 31.38 | Epsilon: 0.100\n",
            "Frames: 4,740,000 | Episode: 928 | Reward(avg100): 32.90 | Epsilon: 0.100\n",
            "Frames: 4,760,000 | Episode: 946 | Reward(avg100): 34.05 | Epsilon: 0.100\n",
            "Frames: 4,780,000 | Episode: 966 | Reward(avg100): 36.72 | Epsilon: 0.100\n",
            "Frames: 4,800,000 | Episode: 985 | Reward(avg100): 36.53 | Epsilon: 0.100\n",
            "Saving checkpoint at 4800000 frames\n",
            "Frames: 4,820,000 | Episode: 1004 | Reward(avg100): 38.30 | Epsilon: 0.100\n",
            "Frames: 4,840,000 | Episode: 1023 | Reward(avg100): 39.33 | Epsilon: 0.100\n",
            "Frames: 4,860,000 | Episode: 1039 | Reward(avg100): 41.37 | Epsilon: 0.100\n",
            "Frames: 4,880,000 | Episode: 1059 | Reward(avg100): 40.30 | Epsilon: 0.100\n",
            "Frames: 4,900,000 | Episode: 1077 | Reward(avg100): 42.03 | Epsilon: 0.100\n",
            "Frames: 4,920,000 | Episode: 1096 | Reward(avg100): 41.20 | Epsilon: 0.100\n",
            "Frames: 4,940,000 | Episode: 1115 | Reward(avg100): 41.11 | Epsilon: 0.100\n",
            "Frames: 4,960,000 | Episode: 1135 | Reward(avg100): 38.61 | Epsilon: 0.100\n",
            "Frames: 4,980,000 | Episode: 1154 | Reward(avg100): 39.13 | Epsilon: 0.100\n",
            "Frames: 5,000,000 | Episode: 1171 | Reward(avg100): 39.00 | Epsilon: 0.100\n",
            "Saving checkpoint at 5000000 frames\n",
            "Training finished.\n"
          ]
        }
      ],
      "source": [
        "buffer = ReplayBuffer(capacity=replay_buffer_size, state_shape=(84, 84, 4))\n",
        "\n",
        "episode_reward_history = []\n",
        "running_reward = 0.0\n",
        "episode_count = 0\n",
        "\n",
        "episode_reward_history = []\n",
        "running_reward = 0.0\n",
        "episode_count = 0\n",
        "starting_framecount = frame_count\n",
        "\n",
        "# @tf.function makes this run much faster by compiling it into a graph\n",
        "@tf.function\n",
        "def train_step(states, next_states, rewards, actions, dones, model, model_target, optimizer, loss_function, gamma):\n",
        "    # Just cast to float32; the model's Rescaling layer (1/255) handles the rest\n",
        "    states = tf.cast(states, tf.float32)\n",
        "    next_states = tf.cast(next_states, tf.float32)\n",
        "\n",
        "    # Double DQN Logic\n",
        "    next_q_online = model(next_states, training=False)\n",
        "    next_actions = tf.argmax(next_q_online, axis=1, output_type=tf.int32)\n",
        "\n",
        "    next_q_target = model_target(next_states, training=False)\n",
        "    indices = tf.stack([tf.range(tf.shape(next_actions)[0]), next_actions], axis=1)\n",
        "    next_q_values = tf.gather_nd(next_q_target, indices)\n",
        "\n",
        "    # Reward clipping (Standard for Atari to keep gradients stable)\n",
        "    clipped_rewards = tf.clip_by_value(rewards, -1.0, 1.0)\n",
        "    targets = clipped_rewards + gamma * (1.0 - dones) * next_q_values\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        q_values = model(states, training=True)\n",
        "        masks = tf.one_hot(actions, num_actions)\n",
        "        q_action = tf.reduce_sum(q_values * masks, axis=1)\n",
        "        loss = loss_function(targets, q_action)\n",
        "\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "# -------------------------\n",
        "# Training Loop\n",
        "# -------------------------\n",
        "while frame_count < total_timesteps:\n",
        "    observation, info = env.reset()\n",
        "    # Ensure state is uint8 to save memory\n",
        "    state = np.array(observation, dtype=np.uint8)\n",
        "    episode_reward = 0\n",
        "\n",
        "    prev_lives = info.get('lives', 5) # Breakout starts with 5 lives\n",
        "\n",
        "    for step in range(1, max_steps_per_episode):\n",
        "        frame_count += 1\n",
        "\n",
        "        # --- Epsilon-Greedy Action ---\n",
        "        if frame_count < learning_starts or np.random.rand() < epsilon:\n",
        "            action = np.random.choice(num_actions)\n",
        "        else:\n",
        "            # Convert to float and normalize just for inference\n",
        "            state_tensor = tf.convert_to_tensor(state)\n",
        "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
        "            state_tensor = tf.cast(state_tensor, tf.float32)\n",
        "\n",
        "            q_values = model(state_tensor, training=False)\n",
        "            action = tf.argmax(q_values[0]).numpy()\n",
        "\n",
        "        # Decay epsilon\n",
        "        epsilon -= epsilon_decay\n",
        "        epsilon = max(epsilon, epsilon_end)\n",
        "\n",
        "        # --- Environment Step ---\n",
        "        state_next, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "        current_lives = info.get('lives', 0)\n",
        "        if current_lives < prev_lives:\n",
        "            # We \"overwrite\" the 0 reward with a -1 penalty\n",
        "            reward = -1.0\n",
        "\n",
        "        # Update lives for the next step comparison\n",
        "        prev_lives = current_lives\n",
        "\n",
        "        done = terminated or truncated\n",
        "        state_next = np.array(state_next, dtype=np.uint8)\n",
        "\n",
        "        episode_reward += reward\n",
        "\n",
        "        # --- Store in Buffer ---\n",
        "        buffer.add(state, action, reward, state_next, done)\n",
        "\n",
        "        state = state_next\n",
        "\n",
        "        # --- Train Step ---\n",
        "        if frame_count > learning_starts + starting_framecount and frame_count % 4 == 0:\n",
        "\n",
        "            state_sample, state_next_sample, rewards_sample, action_sample, done_sample = buffer.sample(batch_size)\n",
        "\n",
        "            # Call the optimized training function\n",
        "            train_step(\n",
        "              state_sample,\n",
        "              state_next_sample,\n",
        "              rewards_sample,\n",
        "              action_sample,\n",
        "              done_sample,\n",
        "              model,\n",
        "              model_target,\n",
        "              optimizer,\n",
        "              loss_function,\n",
        "              gamma\n",
        "          )\n",
        "\n",
        "        # --- Target Network Update ---\n",
        "        if frame_count % target_update_freq == 0:\n",
        "            model_target.set_weights(model.get_weights())\n",
        "            print(f\"Frames: {frame_count:,} | Episode: {episode_count} | \"\n",
        "                  f\"Reward(avg100): {running_reward:.2f} | Epsilon: {epsilon:.3f}\")\n",
        "\n",
        "        # --- Checkpointing ---\n",
        "        if frame_count % 200_000 == 0:\n",
        "            print(f\"Saving checkpoint at {frame_count} frames\")\n",
        "            save_path = f\"/content/drive/MyDrive/checkpoints3/breakout_dqn_{frame_count}.keras\"\n",
        "            model.save(save_path)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # --- Episode End Updates ---\n",
        "    episode_reward_history.append(episode_reward)\n",
        "    if len(episode_reward_history) > 100:\n",
        "        episode_reward_history.pop(0) # Standard list pop is fine here (short list)\n",
        "\n",
        "    running_reward = np.mean(episode_reward_history)\n",
        "    episode_count += 1\n",
        "\n",
        "# Final Save\n",
        "model.save(\"/content/drive/MyDrive/checkpoints3/breakout_dqn_final.keras\")\n",
        "print(\"Training finished.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}